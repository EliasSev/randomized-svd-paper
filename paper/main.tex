\documentclass[conference]{IEEEtran}
%\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\graphicspath{{../figures/}}
\def\BibTeX{
    {\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}
}

\begin{document}

\title{On the Accuracy of Randomized Low-Rank Matrix Approximations}

\author{
    \IEEEauthorblockN{
    Elias J. Severinsen\IEEEauthorrefmark{1},
    Ole L. Elvetun\IEEEauthorrefmark{2}}
    \IEEEauthorblockA{
    \textit{Department of Mathematics} \\
    \textit{Norwegian University of Life Sciences (NMBU)} \\
    Ås, Norway \\
    \{elias.jegervatn.severinsen, ole.elvetun\}@nmbu.no}
}

\maketitle


\begin{abstract}
Low-rank matrix approximations are essential in modern applications like scientific computing and data analysis. In recent years the volume of data has seen a rapid increase, resulting in a need for efficient and accurate low-rank approximations. The computation of the singular value decomposition (SVD) is fundamental within low-rank approximations, but is computationally infeasible as the data size increases. Randomized SVD (rSVD) provides an efficient solution using randomized algorithms to approximate the SVD by first projecting the data onto a low-dimensional space before performing the SVD, significantly reducing the computation time. In this paper I will analyze the computation time on data if wide and tall format, comparing the two cases, and analyzing the error on matrices of different properties.
\end{abstract}


\section{Introduction}
% Motivation: big data
The need for efficient and accurate numerical algorithms for data analysis is more relevant than ever. Modern scientific and industrial applications produce data sets of massive scale and complexity. In physics simulation, discretization of partial differential equations gives linear systems with billions of unknowns \cite{ohana_well_2025}. In medical image analysis, large-scale studies produces data sets with hundreds of thousands of images \cite{bustos_medical_2020}. Similarly, in natural language processing, document-term matrices can have tens of thousands of columns and hundreds of rows \cite{antonellis_exploring_2006}. Indeed, large-scale data sets naturally occur in a wide range of fields and applications.

% Introducing the SVD, its challenges, and the rSVD
The singular value decomposition (SVD) is a fundamental tool in data analysis and numerical linear algebra. It has numerous applications, including dimensionality reduction and high-dimensional data visualization \cite{gewers_principal_2021}, data compression \cite{mathews_image_nodate}, and various regularization techniques \cite{hansen_discrete_2010}. Given a matrix $A\in\mathbb R^{m\times n}$, the SVD decomposes the matrix into its main directions of variation, the singular vectors, and a set of singular values describing the importance of the identified directions. However, the computational cost of the SVD is expensive: for an $n\times n$ matrix, the cost scales as $\mathcal O(n^3)$, which makes it infeasible for many large scale applications. This has motivated the development of randomized algorithms, notably the randomized SVD (rSVD), which provides an efficient alterative at the cost of lower accuracy.

The rSVD computes a low-rank approximation of $A$ by sampling its column space with random test vectors to construct an approximate orthonormal basis $Q$. The matrix is projected onto this approximate subspace, producing a matrix $B$ of smaller size, which captures the main components of $A$. Performing the SVD on this reduced version of $A$ gives an efficient way to compute a rank-$k$ approximation.

% The goal of this paper
In this work, I investigate the trade-offs between the classical SVD and its randomized version. The goal is to compare the computational efficiency and accuracy of the rSVD relative to the SVD on a range of matrix shapes, sizes and spectral decay profiles. I use synthetic test matrices, with a prescribed spectral decay, to investigate the effect on the error of the rSVD against the SVD. In addition, I compare the performance of the rSVD on both tall and wide matrices, evaluating how its efficiency varies with matrix dimensions and target approximation rank.

% Main results
My experiments show that\dots


\section{Theory}\label{sec:theory}


\subsection{Problem formulation}
Consider the goal of finding a rank-$k$ approximation to an $m \times n$ matrix $A$. We are seeking a matrix $X$ with $\text{rank}(X) \le k$ which approximates $A$. The problem formulation is
\begin{equation}\label{probForm1}
    \min_{\text{rank}(X)\le k} \|A - X \|_2
\end{equation}
where $\| \cdot \|_2$ is the $\ell_2$ operator norm, also known as the spectral norm. Equivalently, every rank-$k$ matrix can be viewed as the projection of $A$ onto a $k$-dimensional subspace of $\mathbb R^m$. 

Let $Q\in\mathbb R^{m\times k}$ have orthonormal columns. Then $QQ^TA$ is the orthogonal projection of $A$ onto the $k$-dimensional subspace $\text{span}(Q)$. The problem can also be formulated as
\begin{equation}\label{probForm2}
    \min_{Q^TQ=I_k} \|A-QQ^TA \|_2 
\end{equation}


\subsection{The singular value decomposition}
The singular value decomposition of a matrix $A\in\mathbb R^{m\times n}$ is a factorization of the form
\begin{equation}
    A = U\Sigma V^T
\end{equation}
where $U$ is an $m \times m$ orthogonal matrix, $\Sigma$ is an $m\times n$ diagonal matrix with non-negative values, called the singular values of $A$, along its diagonal, and $V$ is an $n \times n$ orthogonal matrix. The singular values are ordered such that $\sigma_1\ge\sigma_2\ge\dots\ge\sigma_n$. Every matrix has such a factorization, where $\Sigma$ is uniquely determined, but $U$ and $V$ are not \cite{lay_linear_2022}. The columns of $U$ and $V$ are called the left and right singular vectors of $A$, respectively.


\subsection{The randomized SVD}
Randomized singular value decomposition (rSVD) is a technique used to approximate the rank-$k$ approximation of an $m\times n$ matrix without computing the SVD. The core idea is to use random sampling to find a low-rank matrix $Q$ which approximates the range of $A$, project $A$ onto $Q$, and compute the SVD of the projection. The projection $Y=Q^TA$ will with a high probability capture the dominant directions of $A$, and the SVD of $Y$ will yield a good approximation \cite{halko_finding_2011}. This procedure is summarized in Algorithm \ref{alg:rsvd}. Note that the approximation $A\approx QQ^T A$ and the approximate factorization $A\approx U\Sigma V^T$ produced by rSVD have the same error. This can be seen by observing that $QQ^TA = QB = Q\tilde U\Sigma V^T$, and setting $U=Q\tilde U$.

\begin{algorithm}[!t]
\caption{rSVD}
\label{alg:rsvd}
\begin{algorithmic}[1]
    \Require $A\in\mathbb R^{m\times n}$, target rank $k$, oversampling parameter $p$.
    \Ensure Approximate rank-$k$ factorization $U\Sigma V^T$.
    \State Generate a random $n\times(k+p)$ test matrix $\Omega$.
    \State Compute $Y = A\Omega$.
    \State Form an orthonormal basis $Q$ of $Y$.
    \State Set $B = Q^TA$.
    \State Compute the SVD $B = \tilde U\Sigma V^T$.
    \State Set $U = Q\tilde U$. 
\end{algorithmic}
\end{algorithm}

\subsection{Error estimates}
In 1936, C. Eckart and G. Young showed that the solution to \eqref{probForm1} is $X = U_k\Sigma_k V_k^T$, where $U_k$ and $V_k$ consists of the $k$ first left and right singular vectors of $A$, respectively, and $\Sigma_k$ the $k$ first singular values of $A$ \cite{eckart_approximation_1936}. This is known as the Eckart–Young–Mirsky theorem. Alternatively, the solution $Q$ of \eqref{probForm2} consists of the $k$ first left singular values of $A$. This implies that
\begin{equation}\label{svdError}
    \min_{\text{rank}(X)\le k} \|A - X \|_2 = \sigma_{k+1},
\end{equation}
where $\sigma_{k+1}$ is the $(k+1)$th singular value of $A$. Equation \eqref{svdError} says that any rank-$k$ approximation of $A$ can not have an error smaller than $\sigma_{k+1}$.

Let $A\in\mathbb R^{m\times n}$, and pick a target rank $k\ge 2$ and an oversampling parameter $p\ge 2$, where $p+k\le\min\{m,n\}$. Draw a random matrix $\Omega\in\mathbb R^{m\times (k+p)}$, with iid elements from the standard normal distribution, and use Alg. \ref{alg:rsvd} to obtain a rank-$k$ approximation $X = U\Sigma V^T$. Then
\begin{eqnarray}\label{rsvdError}
\mathbb E\|A - X\|_2 \le \gamma_k \sigma_{k+1}, \\
\gamma_k = \left[ 1 + \frac{4 \sqrt{k+p}}{p-1} \sqrt{\min\{m,n\}} \right],
\end{eqnarray}
where $\mathbb E$ denotes the expected value. A proof of this can be found in \cite{halko_finding_2011}. It is always the case that $\gamma_k > 1$, and for \eqref{rsvdError} to give a good upper bound, the singular values $\sigma_k$ must approach zero faster than $\gamma_k$ increases.


\subsection{Computational cost}
Assume that $A\in\mathbb R^{m\times n}$. Standard implementations of the SVD in linear algebra libraries like LAPACK or ScaLAPACK are based on two stage algorithms. In the first stage, $A$ is reduced to a bi-diagonal form through Householder transformations or Gives rotations. In the second stage, the SVD of the bidiagonal matrix if computed, typically using a QR-based or a divide-and-conquer approach. The LAPACK routine GESDD, used in this study, implements the divide-and-conquer method, which typically achieves higher performance compared to the QR-based approach \cite{dongarra_singular_2018}. The cost of this process is of the order $\mathcal O(mn\min\{m,n\})$, which simplifies to $\mathcal O(m^2n)$ for wide matrices where $m < n$, or to $\mathcal O(mn^2)$ for tall matrices where $m > n$.

The rSVD algorithm consists of two stages. In the first stage, an approximate orthogonal basis $Q$ of $A$ is constructed, and in the second stage, the SVD is approximated using the projection of $A$ onto $Q$. The first stage of Algorithm \ref{alg:rsvd}, which constructs the orthogonal basis $Q$, has a cost of $\mathcal O(k^2m)$ as a result of the QR factorization. Stage two, the projection and construction of an approximate SVD, has a cost of $\mathcal O(mnk)$. The overall cost is therefore on the order of $\mathcal O(mnk)$. More efficient but less accurate algorithms for stage 2 are proposed, for instance Algorithm 5.2 in \cite{halko_finding_2011}, which relies on the interpolative decomposition (ID), and has a cost of $\mathcal O(mn\log k)$.


\section{Results}\label{sec:results}


\subsection{Test matrix construction}
\begin{figure}[hbp]
    \centerline{\includegraphics[width=0.50\textwidth]{singular_value_decay.png}}
    \caption{The slow, moderate, and rapid spectral decay used for the test matrix generation. In this figure, for $n=100$.}
    \label{spectralDecay}
\end{figure}
\begin{figure*}[htbp]
    \centerline{\includegraphics[width=\textwidth]{timing_all.png}}
    \caption{Median computation time for SVD and rSVD on wide data $A$ and tall data $A^T$ for increasing $k$ and $p=5$. Top row: time in seconds for SVD and rSVD. Bottom row: SVD computation time divided by rSVD computation time.}
    \label{timeAll}
\end{figure*}

To compare the performance of the SVD and the rSVD on matrices with different spectral decay profiles, I generate random test matrices $A=U\Sigma V^T$, where $U$ and $V$ are random orthogonal matrices, and $\Sigma$ is a diagonal matrix with prescribed singular values $\{\sigma_1, \dots, \sigma_n\}$.

Each orthogonal matrix is constructed by drawing a random $n\times n$ matrix $\Omega$ with independent elements from the standard normal distribution, performing a QR decomposition $\Omega = \tilde QR$, and correcting the signs of the orthogonal columns with $Q = \tilde Q\Lambda$, where $\Lambda = \text{diag}(\text{sign}(R_{ii}))$,  so that the diagonal elements or $R$ are positive. This procedure generates random orthogonal matrices uniformly distributed over the orthogonal group (Haar measure) \cite{mezzadri_how_2007}.

To investigate how spectral decay affects accuracy, three sets of prescribed singular values are used. I use the polynomially decaying sequence of the form
\begin{equation}\label{decay}
\sigma_{k+1} = \frac{10}{(1 + \alpha k)^2},\quad k=0,\dots,n-1,
\end{equation}
where the parameter $\alpha$ controls the rate of decay. The condition number is $\kappa = \sigma_1 / \sigma_{n} = (1 + \alpha n - \alpha)^2$, and solving for $\alpha$ gives
\begin{equation}
\alpha = \frac{\sqrt{\kappa(n-1)^2}-n+1}{(n-1)^2}.
\end{equation}
In this study, I use the condition numbers $\kappa_1 = 2$, $\kappa_2 = 50$, and $\kappa_3 = 1000$, which corresponds to slow, moderate and a rapid spectral decay (see Fig.~\ref{spectralDecay}). The singular values from \eqref{decay} are placed on the diagonal of $\Sigma$, and the test matrix is the result of $A = U\Sigma V^T$.

The size of the test matrices are set to $n\times 5n$, for the values $n_s=100$, $n_m=500$, and $n_l=1000$, which I refer to as the small, medium and large case, respectively. For each combination of spectral decay (slow, moderate, rapid) and matrix size (small, medium, large), I generate $R$ random samples depending on the size, where $R=30$ for the small case, $R=15$ for the medium case, and $R=5$ for the large case. See Table~\ref{tab:matrixCount} for an overview.
\begin{table}[ht]
\centering
\caption{Number of test matrices used for each combination of matrix size and spectral decay.}
\label{tab:matrixCount}
\resizebox{\columnwidth}{!}{
\begin{tabular}{lccc}
    \toprule
    \textbf{Matrix size (dimensions)} & \textbf{Slow decay} & \textbf{Moderate decay} & \textbf{Rapid decay} \\
    \midrule
    Small  ($n_s \times 5n_s$)  & 30 & 30 & 30 \\
    Medium ($n_m \times 5n_m$)  & 15 & 15 & 15 \\
    Large  ($n_l \times 5n_l$)  & 5  & 5  & 5  \\
    \bottomrule
\end{tabular}
}
\end{table}

\begin{figure*}[htbp]
    \centerline{\includegraphics[width=\textwidth]{wide_versus_tall.png}}
    \caption{The median value of $t_{wide}$ divided by $t_{tall}$, which is the time of rSVD on wide data and on tall data, respectively. Here, three different cases: the small dataset (left), the medium dataset (middle) and the large dataset (right).}
    \label{wideVsTall}
\end{figure*}

\begin{table}[ht]
\centering
\caption{Computational time for rSVD relative to SVD at selected target ranks $k$ for small, medium, and large matrices.}
\label{tab:times_all}
\begin{tabular}{ccccccc}
    \toprule
    & \multicolumn{2}{c}{\textbf{Small}} & \multicolumn{2}{c}{\textbf{Medium}} & \multicolumn{2}{c}{\textbf{Large}} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
    \textbf{$k$} & Wide & Tall & Wide & Tall & Wide & Tall \\
    \midrule
    5  & 13.03 & 14.87 & 6.15 & 19.80 & 24.79 & 31.62 \\
    10 & 10.39 & 13.59 & 5.82 & 8.03 & 17.64 & 11.53 \\
    20 & 0.21 & 0.19 & 2.91 & 1.80 & 12.72 & 8.04 \\
    30 & 0.20 & 0.13 & 2.57 & 1.57 & 10.89 & 7.58 \\
    40 & 0.20 & 0.10 & 2.45 & 1.48 & 10.01 & 7.27 \\
    50 & 0.19 & 0.08 & 2.27 & 1.46 & 9.41 & 6.92 \\
    \bottomrule
\end{tabular}
\end{table}


\subsection{Computational Performance}
When assessing the effect of the matrix size on the computational time, I compute the median value across all test matrices of a given size and across all three spectral decay sets, since the condition number of the matrix does not affect the computation cost of the SVD and the rSVD (reference). For each matrix size, I report the median value over 90 (small), 45 (medium), and 15 (large) trials. The results are presented in Fig.~\ref{timeAll}, and relative runtimes for selected target ranks are listed in Tables~\ref{tab:times_all} and \ref{tab:times_tall_wide}.


\subsubsection{Effect of matrix size}
Fig.~\ref{timeAll} shows the absolute run times (top row) and the relative speedup of rSVD over SVD as a function of target rank $k$ (bottom row). For small matrices ($n_s=100$), rSVD is significantly faster than SVD for small ranks ($k<10$). As $k$ increases, the computational time of rSVD increases rapidly, and SVD is faster when $k$ is greater than 10. For medium ($n_m=500$) and large ($n_l=1000$) matrices, rSVD consistently performs faster than SVD across all ranks used in this study (5 to 50). Speedups between 7 and 32 is seen for large matrices and between 1.5 and 20 for medium matrices. The relative advantage of rSVD over SVD generally increases with the matrix size.


\subsubsection{Effect of matrix shape}
To assess the influence of matrix shape, I compare the median computational time of rSVD on wide matrices ($t_{wide}$) versus tall matrices ($t_{tall}$) by looking at the ratio $t_{wide} / t_{tall}$, see Fig~\ref{wideVsTall} and Table~\ref{tab:times_tall_wide}. For small target ranks, rSVD has a faste runtime on wide matrices across all matrix sizes. The crossover, where rSVD becomes faster on tall matrices, decreases with increasing matrix size, occurring at ranks 22, 14, and 8, for small, medium, and large matrices, respectively. For small matrices, once this crossover rank is exceeded, the relative advantage of tall matrices grows as $k$ approaches 50. For medium sized matrices, the relative speedup on tall matrices remains approximately constant once the rank surpasses the crossover point. For large matrices, the advantage of tall matrices diminishes as $k$ increases passed the crossover point.

\begin{table}[t]
\centering
\caption{Relative computational time for wide compared to tall data using rSVD at selected target ranks $k$ for small, medium, and large matrices.}
\label{tab:times_tall_wide}
\begin{tabular}{cccc}
    \toprule
    k & \textbf{Small} & \textbf{Medium} & \textbf{Large} \\
    \midrule
    5  & 0.79 & 0.42 & 0.67 \\
    10 & 0.61 & 0.67 & 1.32 \\
    20 & 0.89 & 1.20 & 1.20 \\
    30 & 1.21 & 1.20 & 1.12 \\
    40 & 1.61 & 1.20 & 1.04 \\
    50 & 1.89 & 1.16 & 1.05 \\
    \bottomrule
\end{tabular}
\end{table}

\begin{figure*}[t]
    \centerline{\includegraphics[width=\textwidth]{errors_all.png}}
    \caption{Average errors for rSVD and best possible error achieved by the SVD over a range of rank-$k$ approximates, for different spectral decays and matrix sizes. Note that the range on the y-axis is different for each subplot. Top row: The $\ell_2$ error for both methods. Bottom row: the relative $\ell_2$ error for rSVD compared to SVD.}
    \label{errorsAll}
\end{figure*}


\subsection{Effect of Spectral Decay on Approximation Error}

\begin{table}[ht]
\centering
\caption{Computational time for rSVD relative to SVD at selected target ranks $k$ for small, medium, and large matrices.}
\label{tab:errors}
\begin{tabular}{cccccccccc}
    \toprule
    & \multicolumn{3}{c}{\textbf{Slow}} & \multicolumn{3}{c}{\textbf{Moderate}} & \multicolumn{3}{c}{\textbf{Rapid}} \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
    \textbf{$k$} & $n_s$ & $n_m$ & $n_l$ & $n_s$ & $n_m$ & $n_l$ & $n_s$ & $n_m$ & $n_l$ \\
    \midrule
    5  & 1.0 & 1.0 & 1.0 & 1.2 & 1.1 & 1.1 & 1.2 & 1.2 & 1.2 \\ 
    10 & 1.1 & 1.0 & 1.0 & 1.5 & 1.2 & 1.1 & 1.6 & 1.5 & 1.3 \\ 
    20 & 1.1 & 1.0 & 1.0 & 1.9 & 1.3 & 1.2 & 2.2 & 1.8 & 1.6 \\ 
    30 & 1.2 & 1.0 & 1.0 & 2.2 & 1.5 & 1.3 & 2.5 & 2.2 & 2.0 \\ 
    40 & 1.2 & 1.1 & 1.0 & 2.4 & 1.6 & 1.4 & 2.6 & 2.5 & 2.2 \\ 
    50 & 1.3 & 1.1 & 1.0 & 2.5 & 1.8 & 1.4 & 2.7 & 2.5 & 2.2 \\ 
    \bottomrule
\end{tabular}
\end{table}
The computation of error is done using the $\ell_2$ operator norm, and using the average value across all test matrices of a given size and spectral decay type. The spectral decay of a matrix does not change if it is transposed, and the rSVD method yield approximately the same results on the transpose of the data. The results are therefore done using the wide data, as the results are equivalent on the transposed case. The results of the experiments can be seen in Fig.~\ref{errorsAll} and Table~\ref{tab:errors}. There are three cases corresponding to the three spectral decay sequences of slow, moderate and rapid decay. For each such case, I again look at the small, medium, and large data matrices, denoted $n_s$, $n_m$, and $n_l$, respectively.

For matrices with slowly decaying singular values, seen in the left column of Fig.~\ref{errorsAll}, the error is larger compared to the moderate and rapidly decaying case. The error is slowly reducing, and staying close to the theoretically lowest achieveable error from the SVD approximations. The error relative to the SVD is smallest for small ranks, and increasing almost linearly. The relative error of rSVD compared to SVD is best for large matrices for all ranks, and worst for small matrices.


\section{Discussion}\label{sec:discussion}
This section contains discussion.

\section{Conclusion}\label{sec:conclusion}
This section contains the conclusion.

\bibliographystyle{plain}  % or 'unsrt', 'alpha', 'ieeetr', etc.
\bibliography{references}

\section*{Appendix}
\paragraph{Operator norm}\label{MatrixNorms}
The $\ell_2$ operator norm of $A\in\mathbb R^{m\times n}$ is the smallest $c\in\mathbb R$ such that $\|Ax\|\le c\|x\|$ for all $x\in \mathbb R^n$, denoted $\|A\|_2$. It is a well known result that
\begin{equation*}
    \|A\|_2 = \sup_{\|x\| = 1}\|Ax\|,
\end{equation*}
for instance, see Lemma 2.7-2 in \cite{kreyszig_functional_1991}.
\paragraph{QR factorization}\label{QRFact}
The QR factorization of $A\in\mathbb R^{m\times b}$ is a decomposition $A = QR$, where $Q\in\mathbb R^{m\times n}$ has orthonormal columns and $R\in\mathbb R^{n\times n}$ is upper diagonal. The matrix $Q$ forms an orthonormal basis for $A$.

\end{document}
