\documentclass[conference]{IEEEtran}
%\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algpseudocode}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\graphicspath{{../figures/}}
\def\BibTeX{
    {\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}
}

\begin{document}

\title{On the Accuracy of Randomized Low-Rank Matrix Approximations}

\author{
    \IEEEauthorblockN{
    Elias J. Severinsen\IEEEauthorrefmark{1},
    Ole L. Elvetun\IEEEauthorrefmark{2}}
    \IEEEauthorblockA{
    \textit{Department of Mathematics} \\
    \textit{Norwegian University of Life Sciences (NMBU)} \\
    Ås, Norway \\
    \{elias.jegervatn.severinsen, ole.elvetun\}@nmbu.no}
}

\maketitle

\begin{abstract}
Low-rank matrix approximations are essential in modern applications like scientific computing and data analysis. In recent years the volume of data has seen a rapid increase, resulting in a need for efficient and accurate low-rank approximations. The computation of the singular value decomposition (SVD) is fundamental within low-rank approximations, but is computationally infeasible as the data size increases. Randomized SVD (rSVD) provides an efficient solution using randomized algorithms to approximate the SVD by first projecting the data onto a low-dimensional space before performing the SVD, significantly reducing the computation time. In this paper I will analyze the computation time on data if wide and tall format, comparing the two cases, and analyzing the error on matrices of different properties.
\end{abstract}

\section{Introduction}
% Motivation: big data
The need for efficient and accurate numerical algorithms for data analysis is more relevant than ever. Modern scientific and industrial applications produce data sets of massive scale and complexity. In physics simulation, discretization of partial differential equations gives linear systems billions of unknowns \cite{ohana_well_2025}. In medical image analysis, large-scale studies produces data sets with hundreds of thousands of images \cite{bustos_medical_2020}. Similarly, in natural language processing, document-term matrices can have tens of thousands of columns and hundreds of rows \cite{antonellis_exploring_2006}. Indeed, large-scale data sets naturally occur in a wide range of fields and applications.

% Introducing the SVD, its challenges, and the rSVD
The singular value decomposition (SVD) is a fundamental tool in data analysis and numerical linear algebra. It has numerous applications, including dimensionality reduction and high-dimensional data visualization \cite{gewers_principal_2021}, data compression \cite{mathews_image_nodate}, and various regularization techniques \cite{hansen_discrete_2010}. Given a matrix $A\in\mathbb R^{m\times n}$, the SVD decomposes the matrix into its main directions of variation, the singular vectors, and a set of singular values describing the importance of the identified directions. However, the computational cost of the SVD is expensive: for an $n\times n$ matrix, the cost scales as $\mathcal O(n^3)$, which makes it infeasible for many large scale applications. This has motivated the development of randomized algorithms, notably the randomized SVD (rSVD), which provides an efficient alterative at the cost of lower accuracy.

The rSVD computes a low-rank approximation of $A$ by sampling its column space with random test vectors to construct an approximate orthonormal basis $Q$. The matrix is projected onto this approximate subspace, producing a matrix $B$ of smaller size, which captures the main components of $A$. Performing the SVD on this reduced version of $A$ gives an efficient way to compute a rank-$k$ approximation.

% The goal of this paper
In this work, I investigate the trade-offs between the classical SVD and its randomized version. The goal is to compare the computational efficiency and accuracy of the rSVD relative to the SVD on a range of matrix shapes, sizes and spectral decay profiles. I use synthetic test matrices, with a prescribed spectral decay, to investigate the effect on the error of the rSVD against the SVD. In addition, I compare the performance of the rSVD on both tall and wide matrices, evaluating how its efficiency varies with matrix dimensions and target approximation rank.

% Main results
My experiments show that\dots

\section{Theory}\label{sec:theory}

\subsection{Problem formulation}
Consider the goal of finding a rank-$k$ approximation to an $m \times n$ matrix $A$. We are seeking a matrix $X$ with $\text{rank}(X) \le k$ which approximates $A$. The problem formulation is
\begin{equation}\label{probForm1}
    \min_{\text{rank}(X)\le k} \|A - X \|_2
\end{equation}
where $\| \cdot \|_2$ is the $\ell_2$ operator norm, also known as the spectral norm. Equivalently, every rank-$k$ matrix can be viewed as the projection of $A$ onto a $k$-dimensional subspace of $\mathbb R^m$. 

Let $Q\in\mathbb R^{m\times k}$ have orthonormal columns. Then $QQ^TA$ is the orthogonal projection of $A$ onto the $k$-dimensional subspace $\text{span}(Q)$. The problem can also be formulated as
\begin{equation}\label{probForm2}
    \min_{Q^TQ=I_k} \|A-QQ^TA \|_2 
\end{equation}

\subsection{The singular value decomposition}
The singular value decomposition of a matrix $A\in\mathbb R^{m\times n}$ is a factorization of the form
\begin{equation}
    A = U\Sigma V^T
\end{equation}
where $U$ is an $m \times m$ orthogonal matrix, $\Sigma$ is an $m\times n$ diagonal matrix with non-negative values, called the singular values of $A$, along its diagonal, and $V$ is an $n \times n$ orthogonal matrix. The singular values are ordered such that $\sigma_1\ge\sigma_2\ge\dots\ge\sigma_n$. Every matrix has such a factorization, where $\Sigma$ is uniquely determined, but $U$ and $V$ are not \cite{lay_linear_2022}. The columns of $U$ and $V$ are called the left and right singular vectors of $A$, respectively.

\subsection{The randomized SVD}

Randomized singular value decomposition (rSVD) is a technique used to approximate the rank-$k$ approximation of an $m\times n$ matrix without computing the SVD. The core idea is to use random sampling to find a low-rank matrix $Q$ which approximates the range of $A$, project $A$ onto $Q$, and compute the SVD of the projection. The projection $Y=Q^TA$ will with a high probability capture the dominant directions of $A$, and the SVD of $Y$ will yield a good approximation \cite{halko_finding_2011}. This procedure is summarized in Algorithm \ref{alg:rsvd}. Note that the approximation $A\approx QQ^T A$ and the approximate factorization $A\approx U\Sigma V^T$ produced by rSVD have the same error. This can be seen by observing that $QQ^TA = QB = Q\tilde U\Sigma V^T$, and setting $U=Q\tilde U$.

\begin{algorithm}[!t]
\caption{rSVD}
\label{alg:rsvd}
\begin{algorithmic}[1]
    \Require $A\in\mathbb R^{m\times n}$, target rank $k$, oversampling parameter $p$.
    \Ensure Approximate rank-$k$ factorization $U\Sigma V^T$.
    \State Generate a random $n\times(k+p)$ test matrix $\Omega$.
    \State Compute $Y = A\Omega$.
    \State Form an orthonormal basis $Q$ of $Y$.
    \State Set $B = Q^TA$.
    \State Compute the SVD $B = \tilde U\Sigma V^T$.
    \State Set $U = Q\tilde U$. 
\end{algorithmic}
\end{algorithm}

\subsection{Error estimates}
In 1936, C. Eckart and G. Young showed that the solution to \eqref{probForm1} is $X = U_k\Sigma_k V_k^T$, where $U_k$ and $V_k$ consists of the $k$ first left and right singular vectors of $A$, respectively, and $\Sigma_k$ the $k$ first singular values of $A$ \cite{eckart_approximation_1936}. This is known as the Eckart–Young–Mirsky theorem. Alternatively, the solution $Q$ of \eqref{probForm2} consists of the $k$ first left singular values of $A$. This implies that
\begin{equation}\label{svdError}
    \min_{\text{rank}(X)\le k} \|A - X \|_2 = \sigma_{k+1},
\end{equation}
where $\sigma_{k+1}$ is the $(k+1)$th singular value of $A$. Equation \eqref{svdError} says that any rank-$k$ approximation of $A$ can not have an error smaller than $\sigma_{k+1}$.

Let $A\in\mathbb R^{m\times n}$, and pick a target rank $k\ge 2$ and an oversampling parameter $p\ge 2$, where $p+k\le\min\{m,n\}$. Draw a random matrix $\Omega\in\mathbb R^{m\times (k+p)}$, with iid elements from the standard normal distribution, and use Alg. \ref{alg:rsvd} to obtain a rank-$k$ approximation $X = U\Sigma V^T$. Then
\begin{eqnarray}\label{rsvdError}
\mathbb E\|A - X\|_2 \le \gamma_k \sigma_{k+1}, \\
\gamma_k = \left[ 1 + \frac{4 \sqrt{k+p}}{p-1} \sqrt{\min\{m,n\}} \right],
\end{eqnarray}
where $\mathbb E$ denotes the expected value. A proof of this can be found in \cite{halko_finding_2011}. It is always the case that $\gamma_k > 1$, and for \eqref{rsvdError} to give a good upper bound, the singular values $\sigma_k$ must approach zero faster than $\gamma_k$ increases.



\subsection{Computational cost}
Assume that $A\in\mathbb R^{m\times n}$. Standard implementations of the SVD in linear algebra libraries like LAPACK or ScaLAPACK are based on two stage algorithms. In the first stage, $A$ is reduced to a bi-diagonal form through Householder transformations or Gives rotations. In the second stage, the SVD of the bidiagonal matrix if computed, typically using a QR-based or a divide-and-conquer approach. The LAPACK routine GESDD, used in this study, implements the divide-and-conquer method, which typically achieves higher performance compared to the QR-based approach \cite{dongarra_singular_2018}. The cost of this process is of the order $\mathcal O(mn\min\{m,n\})$, which simplifies to $\mathcal O(m^2n)$ for wide matrices where $m < n$, or to $\mathcal O(mn^2)$ for tall matrices where $m > n$.

The rSVD algorithm consists of two stages. In the first stage, an approximate orthogonal basis $Q$ of $A$ is constructed, and in the second stage, the SVD is approximated using the projection of $A$ onto $Q$. The first stage of Algorithm \ref{alg:rsvd}, which constructs the orthogonal basis $Q$, has a cost of $\mathcal O(k^2m)$ as a result of the QR factorization. Stage two, the projection and construction of an approximate SVD, has a cost of $\mathcal O(mnk)$. The overall cost is therefore on the order of $\mathcal O(mnk)$. More efficient but less accurate algorithms for stage 2 are proposed, for instance Algorithm 5.2 in \cite{halko_finding_2011}, which relies on the interpolative decomposition (ID), and has a cost of $\mathcal O(mn\log k)$.


\section{Results}\label{sec:results}

\subsection{Test matrix construction}
To compare the performance of the SVD and the rSVD on data with a specified decay of singular values, I generate random test matrices $A=U\Sigma V^T$, where $U$ and $V$ are random orthogonal matrices, and $\Sigma$ is a diagonal matrix with prescribed singular values $\{\sigma_1, \dots, \sigma_n\}$.

Each orthogonal matrix is constructed by drawing a random $n\times n$ matrix $\Omega$ with independent elements from the standard normal distribution, performing a QR decomposition $\Omega = \tilde QR$, and correcting the signs of the orthogonal columns with $Q = \tilde Q\Lambda$, where $\Lambda = \text{diag}(\text{sign}(R_{ii}))$,  so that the diagonal elements or $R$ are positive. This procedure generates random orthogonal matrices uniformly distributed over the orthogonal group (Haar measure) \cite{mezzadri_how_2007}.

To investigate how spectral decay affects accuracy, three sets of prescribed singular values are used. I use the polynomially decaying sequence of the form
\begin{equation}
\sigma_{k+1} = \frac{10}{(1 + \alpha k)^2},\quad k=0,\dots,n-1,
\end{equation}
where the parameter $\alpha$ controls the rate of decay. The condition number is $\kappa = \sigma_1 / \sigma_{n} = (1 + \alpha n - \alpha)^2$, and solving for $\alpha$ gives
\begin{equation}
\alpha = \frac{\sqrt{\kappa(n-1)^2}-n+1}{(n-1)^2}.
\end{equation}
In this study, I use the condition numbers $\kappa = 3,\space 100,\space 1000$, which corresponds to slow, moderate and a rapid spectral decay (see Fig.~\ref{spectralDecay}).

\begin{figure}[tbp]
    \centerline{\includegraphics[width=0.50\textwidth]{singular_value_decay.png}}
    \caption{The slow, moderate, and rapid spectral decay used for the test matrix generation. In this figure, for $n=100$.}
    \label{spectralDecay}
\end{figure}

\subsection{Computation times}
This section contains results on the computation time on rSVD and SVD.

\begin{figure*}[tbp]
    \centerline{\includegraphics[width=\textwidth]{wide_versus_tall.png}}
    \caption{The median value of $t_{wide}$ divided by $t_{tall}$, which is the time of rSVD on wide data and on tall data, respectively. Here, three different cases: the small dataset (left), the medium dataset (middle) and the large dataset (right).}
    \label{wideVsTall}
\end{figure*}

\begin{figure}[tbp]
    \centerline{\includegraphics[width=0.5\textwidth]{timing_all.png}}
    \caption{Median computation time for SVD and rSVD on wide data $A$ of size $n\times 5n$, and on tall data $A^T$ of size $5n \times n$, for increasing $k$. Median taken over $R$ runs, depending on the data size. Top: $n=100$ and $R=30$. Middle: $n=500$ and $R=10$. Bottom: $n=1000$ and $R=5$.}
    \label{timeAll}
\end{figure}

\begin{figure}[tbp]
    \centerline{\includegraphics[width=0.5\textwidth]{speedups_all.png}}
    \caption{Median speedup of rSVD compared to SVD for tall and wide data for small, medium and large matrices.}
    \label{speedUp}
\end{figure}


\subsection{Errors}
This section contains results on the errors produced by the rSVD and the SVD.

\begin{figure}[tbp]
    \centerline{\includegraphics[width=0.5\textwidth]{errors_raw.png}}
    \caption{Average errors for rSVD and best possible error achieved by the SVD for different spectral decays and matrix sizes. The y-axis shows the $\ell_2$ operator norm, and the x-axis is the approximation rank $k$.}
    \label{errorsRaw}
\end{figure}

\begin{figure}[tbp]
    \centerline{\includegraphics[width=0.5\textwidth]{errors_relative.png}}
    \caption{The relative error of rSVD, in $\%$, compared to SVD, for different spectral decays and matrix sizes. The y-axis shows the $\%$-difference in $\ell_2$ operator norm, and the x-axis is the approximation rank $k$.}
    \label{errorsRelative}
\end{figure}


\subsection{Error estimates}

On errors related to SVD and rSVD.


\section{Discussion}\label{sec:discussion}
This section contains discussion.

\section{Conclusion}\label{sec:conclusion}
This section contains the conclusion.

\bibliographystyle{plain}  % or 'unsrt', 'alpha', 'ieeetr', etc.
\bibliography{references}

\section*{Appendix}
\paragraph{Operator norm}\label{MatrixNorms}
The $\ell_2$ operator norm of $A\in\mathbb R^{m\times n}$ is the smallest $c\in\mathbb R$ such that $\|Ax\|\le c\|x\|$ for all $x\in \mathbb R^n$, denoted $\|A\|_2$. It is a well known result that
\begin{equation}
    \|A\|_2 = \sup_{\|x\| = 1}\|Ax\|,
\end{equation}
for instance, see Lemma 2.7-2 in \cite{kreyszig_functional_1991}.
\paragraph{QR factorization}\label{QRFact}
The QR factorization of $A\in\mathbb R^{m\times b}$ is a decomposition $A = QR$, where $Q\in\mathbb R^{m\times n}$ has orthonormal columns and $R\in\mathbb R^{n\times n}$ is upper diagonal.

\section*{Tips}
I will remove this section later.

\paragraph{Tips regarding figures and tables}
Place figures and tables at the top and bottom of columns. Avoid placing them in the middle of columns. Figure captions should be 
below the figures. Insert figures and tables after they are cited in the text. Use the abbreviation 
``Fig.~\ref{spectralDecay}'', even at the beginning of a sentence.


\end{document}
