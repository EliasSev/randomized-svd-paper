\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\graphicspath{{../figures/}}
\def\BibTeX{
    {\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}
}

\begin{document}

\title{On the Accuracy of Randomized Low-Rank Matrix Approximations}

\author{
    \IEEEauthorblockN{
    Elias J. Severinsen\IEEEauthorrefmark{1},
    Ole L. Elvetun\IEEEauthorrefmark{2}}
    \IEEEauthorblockA{
    \textit{Department of Mathematics} \\
    \textit{Norwegian University of Life Sciences (NMBU)} \\
    Ås, Norway \\
    \{elias.jegervatn.severinsen, ole.elvetun\}@nmbu.no}
}

\maketitle

\begin{abstract}
Low-rank matrix approximations are essential in modern applications like scientific computing and data analysis. In recent years the volume of data has seen a rapid increase, resulting in a need for efficient and accurate low-rank approximations. The computation of the singular value decomposition (SVD) is fundamental within low-rank approximations, but is computationally infeasible as the data size increases. Randomized SVD (rSVD) provides an efficient solution using randomized algorithms to approximate the SVD by first projecting the data onto a low-dimensional space before performing the SVD, significantly reducing the computation time. In this paper I will analyze the computation time and error of rSVD, comparing it with theoretical results and with the SVD.
\end{abstract}

\section{Introduction}
This is the introduction.

\section{Theory}\label{TH}

\subsection{Problem formulation}
Consider the goal of finding a rank-$k$ approximation to an $m \times n$ matrix $A$. We are seeking a matrix $X$ with $\text{rank}(X) \le k$ which approximates $A$. The problem formulation is
\begin{equation}\label{probForm1}
\min_{\text{rank}(X)\le k} \|A - X \|_2
\end{equation}
where $\| \cdot \|_2$ is the $\ell_2$ operator norm. Equivalently, every rank-$k$ matrix can be viewed as the projection of $A$ onto a $k$-dimensional subspace of $\mathbb R^m$. 

Let $Q\in\mathbb R^{m\times k}$ have orthonormal columns. Then $QQ^TA$ is the orthogonal projection of $A$ onto the $k$-dimensional subspace $\text{span}(Q)$. The problem can also be formulated as
\begin{equation}\label{probForm2}
\min_{Q^TQ=I_k} \|A-QQ^TA \|_2 
\end{equation}

\subsection{The singular value decomposition}
The singular value decomposition of a matrix $A\in\mathbb R^{m\times n}$ is a factorization of the form
$$
A = U\Sigma V^T
$$
where $U$ is an $m \times m$ orthogonal matrix, $\Sigma$ is an $m\times n$ diagonal matrix with non-negative values, called the singular values of $A$, along its diagonal, and $V$ is an $n \times n$ orthogonal matrix. Every matrix has such a factorization, where $\Sigma$ is uniquely determined, but $U$ and $V$ are not \cite{lay_linear_2022}. The columns of $U$ and $V$ are called the left and right singular vectors of $A$, respectively. 

In a 1936 paper, C. Eckart and G. Young showed that the solution to \eqref{probForm1} is $X = U_k\Sigma_k V_k^T$, where $U_k$ and $V_k$ consists of the $k$ first left and right singular vectors of $A$, respectively, and $\Sigma_k$ the $k$ first singular values of $A$ \cite{eckart_approximation_1936}. This is now known as the Eckart–Young–Mirsky theorem. Alternatively, the solution $Q$ of \eqref{probForm2} consists of the $k$ first left singular values of $A$.

\subsection{The randomized SVD}
Randomized singular value decomposition (rSVD) is a technique used to approximate the $k$-rank approximation of an $m\times n$ matrix  without computing the SVD. The core idea is to use random sampling of  to find a low-rank matrix $Q$ which approximates the range of $A$.

Halko et al proposed the Proto-Algorithm for computing the rSVD. Pick a target rank $k$ and an oversampling parameter $p$ and draw a $m\times (k+p)$ random test matrix $\Omega$. 

Next, let
$$
Y = A\Omega
$$
it can be shown that with a high probability span the dominant subspace of $A$ (reference). Then, compute the orthogonal basis $Q$ of $Y$, and form the projection $B = Q^TA$. We can now compute the SVD of $B$,
$$
B = \tilde U\Sigma V^T
$$
Since $A \approx QQ^TA = QB = Q\tilde U\Sigma V^T$, letting $U = Q\tilde U$ gives the low-rank approximation $A \approx U\Sigma V^T$.

\section{Results}\label{RE}
This section contains results.

\subsection{Singular value decomposition}
Results on SVD.

\subsection{Randomized SVD}
Results on rSVD,

\begin{equation}\label{rSVD}
A \approx (Q\tilde{U})\Sigma V^T = U\Sigma V^T
\end{equation}

Reference to \eqref{rSVD}.

\subsection{Error estimates}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.5\textwidth]{SVD_vs_rSVD.jpg}}
\caption{The four fundamental subspaces of linear algebra.}
\label{SVD_vs_rSVD}
\end{figure}

On errors related to SVD and rSVD.

\begin{equation}
E\|A - QQ^T A\| \le
\left[ 1 + \frac{4 \sqrt{k+p}}{p-1} \sqrt{\min\{m,n\}} \right]
\sigma_{k+1}
\end{equation}

This is an upper bound on the error in rSVD, see \cite{halko_finding_2011}.

\section{Discussion}\label{DI}
This section contains discussion.

\section{Conclusion}\label{CO}
This section contains the conclusion.

\section*{Acknowledgment}
Tips: Avoid the stilted expression ``one of us (R. B. 
G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
acknowledgments in the unnumbered footnote on the first page.

\bibliographystyle{plain}  % or 'unsrt', 'alpha', 'ieeetr', etc.
\bibliography{references}

\section*{Appendix}
\paragraph{Matrix norms}\label{MatrixNorms}
The $\ell_2$ norm of $A\in\mathbb R^{m\times n}$ is given by
$\|A\|_2 = \sup_{\|x\|=1} \|Ax\|$ and the Frobenius norm is given by
\begin{equation*}
\|A\|_F = \sqrt{\sum_{i=1}^m\sum_{j=1}^n|a_{ij}|^2}
\end{equation*}

\paragraph{QR factorization}\label{QRFact}
The QR factorization of $A\in\mathbb R^{m\times b}$ is a decomposition $A = QR$, where $Q\in\mathbb R^{m\times n}$ has orthonormal columns and $R\in\mathbb R^{n\times n}$ is upper diagonal.

\paragraph{Tips regarding figures and tables}
Place figures and tables at the top and bottom of columns. Avoid placing them in the middle of columns. Figure captions should be 
below the figures. Insert figures and tables after they are cited in the text. Use the abbreviation 
``Fig.~\ref{SVD_vs_rSVD}'', even at the beginning of a sentence.



\end{document}
