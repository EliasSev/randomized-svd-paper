\documentclass[conference]{IEEEtran}
%\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\graphicspath{{../figures/}}
\def\BibTeX{
    {\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}
}

\begin{document}

\title{On the Accuracy of Randomized Low-Rank Matrix Approximations}

\author{
    \IEEEauthorblockN{
    Elias J. Severinsen\IEEEauthorrefmark{1},
    Ole L. Elvetun\IEEEauthorrefmark{2}}
    \IEEEauthorblockA{
    \textit{Department of Mathematics} \\
    \textit{Norwegian University of Life Sciences (NMBU)} \\
    Ås, Norway \\
    \{elias.jegervatn.severinsen, ole.elvetun\}@nmbu.no}
}

\maketitle


\begin{abstract}
Low-rank matrix approximations are essential in modern applications like scientific computing and data analysis. In recent years the volume of data has seen a rapid increase, resulting in a need for efficient and accurate low-rank approximations. The computation of the singular value decomposition (SVD) is fundamental within low-rank approximations, but is computationally infeasible as the data size increases. Randomized SVD (rSVD) provides an efficient solution using randomized algorithms to approximate the SVD by first projecting the data onto a low-dimensional space before performing the SVD, significantly reducing the computation time. In this paper I will analyze the computation time on data if wide and tall format, comparing the two cases, and analyzing the error on matrices of different properties.
\end{abstract}


\section{Introduction}
% Motivation: big data
The need for efficient and accurate numerical algorithms for data analysis is more relevant than ever. Modern scientific and industrial applications produce data sets of massive scale and complexity. In physics simulation, discretization of partial differential equations gives linear systems with billions of unknowns \cite{ohana_well_2025}. In medical image analysis, large-scale studies produces data sets with hundreds of thousands of images \cite{bustos_medical_2020}. Similarly, in natural language processing, document-term matrices can have tens of thousands of columns and hundreds of rows \cite{antonellis_exploring_2006}. Indeed, large-scale data sets naturally occur in a wide range of fields and applications.

% Introducing the SVD, its challenges, and the rSVD
The singular value decomposition (SVD) is a fundamental tool in data analysis and numerical linear algebra. It has numerous applications, including dimensionality reduction and high-dimensional data visualization \cite{gewers_principal_2021}, data compression \cite{mathews_image_nodate}, and various regularization techniques \cite{hansen_discrete_2010}. Given a matrix $A\in\mathbb R^{m\times n}$, the SVD decomposes the matrix into its main directions of variation, the singular vectors, and a set of singular values describing the importance of the identified directions. However, the computational cost of the SVD is expensive: for an $n\times n$ matrix, the cost scales as $\mathcal O(n^3)$, which makes it infeasible for many large scale applications. This has motivated the development of randomized algorithms, notably the randomized SVD (rSVD), which provides an efficient alterative at the cost of lower accuracy.

The rSVD computes a low-rank approximation of $A$ by sampling its column space with random test vectors to construct an approximate orthonormal basis $Q$. The matrix is projected onto this approximate subspace, producing a matrix $B$ of smaller size, which captures the main components of $A$. Performing the SVD on this reduced version of $A$ gives an efficient way to compute a rank-$k$ approximation.

% The goal of this paper
In this work, I investigate the trade-offs between the classical SVD and its randomized version. The goal is to compare the computational efficiency and accuracy of the rSVD relative to the SVD on a range of matrix shapes, sizes and spectral decay profiles. I use synthetic test matrices, with a prescribed spectral decay, to investigate the effect on the error of the rSVD against the SVD. In addition, I compare the performance of the rSVD on both tall and wide matrices, evaluating how its efficiency varies with matrix dimensions and target approximation rank.

% Main results
My experiments show that\dots


\section{Theory}\label{sec:theory}


\subsection{Problem formulation}
Consider the goal of finding a rank-$k$ approximation to an $m \times n$ matrix $A$. We are seeking a matrix $X$ with $\text{rank}(X) \le k$ which approximates $A$. The problem formulation is
\begin{equation}\label{probForm1}
    \min_{\text{rank}(X)\le k} \|A - X \|_2
\end{equation}
where $\| \cdot \|_2$ is the $\ell_2$ operator norm, also known as the spectral norm. Equivalently, every rank-$k$ matrix can be viewed as the projection of $A$ onto a $k$-dimensional subspace of $\mathbb R^m$. 

Let $Q\in\mathbb R^{m\times k}$ have orthonormal columns. Then $QQ^TA$ is the orthogonal projection of $A$ onto the $k$-dimensional subspace $\text{span}(Q)$. The problem can also be formulated as
\begin{equation}\label{probForm2}
    \min_{Q^TQ=I_k} \|A-QQ^TA \|_2 
\end{equation}


\subsection{The singular value decomposition}
The singular value decomposition of a matrix $A\in\mathbb R^{m\times n}$ is a factorization of the form
\begin{equation}
    A = U\Sigma V^T
\end{equation}
where $U$ is an $m \times m$ orthogonal matrix, $\Sigma$ is an $m\times n$ diagonal matrix with non-negative values, called the singular values of $A$, along its diagonal, and $V$ is an $n \times n$ orthogonal matrix. The singular values are ordered such that $\sigma_1\ge\sigma_2\ge\dots\ge\sigma_n$. Every matrix has such a factorization, where $\Sigma$ is uniquely determined, but $U$ and $V$ are not \cite{lay_linear_2022}. The columns of $U$ and $V$ are called the left and right singular vectors of $A$, respectively.


\subsection{The randomized SVD}
Randomized singular value decomposition (rSVD) is a technique used to approximate the rank-$k$ approximation of an $m\times n$ matrix without computing the SVD. The core idea is to use random sampling to find a low-rank matrix $Q$ which approximates the range of $A$, project $A$ onto $Q$, and compute the SVD of the projection. The projection $Y=Q^TA$ will with a high probability capture the dominant directions of $A$, and the SVD of $Y$ will yield a good approximation \cite{halko_finding_2011}. This procedure is summarized in Algorithm \ref{alg:rsvd}. Note that the approximation $A\approx QQ^T A$ and the approximate factorization $A\approx U\Sigma V^T$ produced by rSVD have the same error. This can be seen by observing that $QQ^TA = QB = Q\tilde U\Sigma V^T$, and setting $U=Q\tilde U$.

\begin{algorithm}[!t]
\caption{rSVD}
\label{alg:rsvd}
\begin{algorithmic}[1]
    \Require $A\in\mathbb R^{m\times n}$, target rank $k$, oversampling parameter $p$.
    \Ensure Approximate rank-$k$ factorization $U\Sigma V^T$.
    \State Generate a random $n\times(k+p)$ test matrix $\Omega$.
    \State Compute $Y = A\Omega$.
    \State Form an orthonormal basis $Q$ of $Y$.
    \State Set $B = Q^TA$.
    \State Compute the SVD $B = \tilde U\Sigma V^T$.
    \State Set $U = Q\tilde U$. 
\end{algorithmic}
\end{algorithm}

\subsection{Error estimates}
In 1936, C. Eckart and G. Young showed that the solution to \eqref{probForm1} is $X = U_k\Sigma_k V_k^T$, where $U_k$ and $V_k$ consists of the $k$ first left and right singular vectors of $A$, respectively, and $\Sigma_k$ the $k$ first singular values of $A$ \cite{eckart_approximation_1936}. This is known as the Eckart–Young–Mirsky theorem. Alternatively, the solution $Q$ of \eqref{probForm2} consists of the $k$ first left singular values of $A$. This implies that
\begin{equation}\label{svdError}
    \min_{\text{rank}(X)\le k} \|A - X \|_2 = \sigma_{k+1},
\end{equation}
where $\sigma_{k+1}$ is the $(k+1)$th singular value of $A$. Equation \eqref{svdError} says that any rank-$k$ approximation of $A$ can not have an error smaller than $\sigma_{k+1}$.

Let $A\in\mathbb R^{m\times n}$, and pick a target rank $k\ge 2$ and an oversampling parameter $p\ge 2$, where $p+k\le\min\{m,n\}$. Draw a random matrix $\Omega\in\mathbb R^{m\times (k+p)}$, with iid elements from the standard normal distribution, and use Alg. \ref{alg:rsvd} to obtain a rank-$k$ approximation $X = U\Sigma V^T$. Then
\begin{eqnarray}\label{rsvdError}
\mathbb E\|A - X\|_2 \le \gamma_k \sigma_{k+1}, \\
\gamma_k = \left[ 1 + \frac{4 \sqrt{k+p}}{p-1} \sqrt{\min\{m,n\}} \right],
\end{eqnarray}
where $\mathbb E$ denotes the expected value. A proof of this can be found in \cite{halko_finding_2011}. It is always the case that $\gamma_k > 1$, and for \eqref{rsvdError} to give a good upper bound, the singular values $\sigma_k$ must approach zero faster than $\gamma_k$ increases.


\subsection{Computational cost}
Assume that $A\in\mathbb R^{m\times n}$. Standard implementations of the SVD in linear algebra libraries like LAPACK or ScaLAPACK are based on two stage algorithms. In the first stage, $A$ is reduced to a bi-diagonal form through Householder transformations or Gives rotations. In the second stage, the SVD of the bidiagonal matrix if computed, typically using a QR-based or a divide-and-conquer approach. The LAPACK routine GESDD, used in this study, implements the divide-and-conquer method, which typically achieves higher performance compared to the QR-based approach \cite{dongarra_singular_2018}. The cost of this process is of the order $\mathcal O(mn\min\{m,n\})$, which simplifies to $\mathcal O(m^2n)$ for wide matrices where $m < n$, or to $\mathcal O(mn^2)$ for tall matrices where $m > n$.

The rSVD algorithm consists of two stages. In the first stage, an approximate orthogonal basis $Q$ of $A$ is constructed, and in the second stage, the SVD is approximated using the projection of $A$ onto $Q$. The first stage of Algorithm \ref{alg:rsvd}, which constructs the orthogonal basis $Q$, has a cost of $\mathcal O(k^2m)$ as a result of the QR factorization. Stage two, the projection and construction of an approximate SVD, has a cost of $\mathcal O(mnk)$. The overall cost is therefore on the order of $\mathcal O(mnk)$. More efficient but less accurate algorithms for stage 2 are proposed, for instance Algorithm 5.2 in \cite{halko_finding_2011}, which relies on the interpolative decomposition (ID), and has a cost of $\mathcal O(mn\log k)$.


\section{Results}\label{sec:results}


\subsection{Test matrix construction}
\input{figures/spectral_decay.tex}
\input{figures/timing_all.tex}

To compare the performance of the SVD and the rSVD on matrices with different spectral decay profiles, I generate random test matrices $A=U\Sigma V^T$, where $U$ and $V$ are random orthogonal matrices, and $\Sigma$ is a diagonal matrix with prescribed singular values $\{\sigma_1, \dots, \sigma_n\}$.

Each orthogonal matrix is constructed by drawing a random $n\times n$ matrix $\Omega$ with independent elements from the standard normal distribution, performing a QR decomposition $\Omega = \tilde QR$, and correcting the signs of the orthogonal columns with $Q = \tilde Q\Lambda$, where $\Lambda = \text{diag}(\text{sign}(R_{ii}))$,  so that the diagonal elements or $R$ are positive. This procedure generates random orthogonal matrices uniformly distributed over the orthogonal group (Haar measure) \cite{mezzadri_how_2007}.

To investigate how spectral decay affects accuracy, three sets of prescribed singular values are used. I use the polynomially decaying sequence of the form
\begin{equation}\label{decay}
\sigma_{k+1} = \frac{10}{(1 + \alpha k)^2},\quad k=0,\dots,n-1,
\end{equation}
where the parameter $\alpha$ controls the rate of decay. The condition number is $\kappa = \sigma_1 / \sigma_{n} = (1 + \alpha n - \alpha)^2$, and solving for $\alpha$ gives
\begin{equation}
\alpha = \frac{\sqrt{\kappa(n-1)^2}-n+1}{(n-1)^2}.
\end{equation}
In this study, I use the condition numbers $\kappa_1 = 2$, $\kappa_2 = 50$, and $\kappa_3 = 1000$, which corresponds to slow, moderate and a rapid spectral decay (see Fig.~\ref{spectralDecay}). The singular values from \eqref{decay} are placed on the diagonal of $\Sigma$, and the test matrix is the result of $A = U\Sigma V^T$.

The size of the test matrices are set to $n\times 5n$, for the values $n_s=100$, $n_m=500$, and $n_l=1000$, which I refer to as the small, medium and large case, respectively. For each combination of spectral decay (slow, moderate, rapid) and matrix size (small, medium, large), I generate $R$ random samples depending on the size, where $R=30$ for the small case, $R=15$ for the medium case, and $R=5$ for the large case. See Table~\ref{tab:matrixCount} for an overview.

\input{tables/matrix_count.tex}
\input{figures/wide_vs_tall.tex}
\input{tables/times_all.tex}


\subsection{Computational Performance}
When assessing the effect of the matrix size on the computational time, I compute the median value across all test matrices of a given size and across all three spectral decay sets, since the condition number of the matrix does not affect the computation cost of the SVD and the rSVD (reference). For each matrix size, I report the median value over 90 (small), 45 (medium), and 15 (large) trials. The results are presented in Fig.~\ref{timeAll}, and relative runtimes for selected target ranks are listed in Tables~\ref{tab:times_all} and \ref{tab:times_tall_wide}.


\subsubsection{Effect of matrix size}
Fig.~\ref{timeAll} shows the absolute run times (top row) and the relative speedup of rSVD over SVD as a function of target rank $k$ (bottom row). For small matrices ($n_s=100$), rSVD is significantly faster than SVD for small ranks ($k<10$). As $k$ increases, the computational time of rSVD increases rapidly, and SVD is faster when $k$ is greater than 10. For medium ($n_m=500$) and large ($n_l=1000$) matrices, rSVD consistently performs faster than SVD across all ranks used in this study (5 to 50). Speedups between 7 and 32 is seen for large matrices and between 1.5 and 20 for medium matrices. The relative advantage of rSVD over SVD generally increases with the matrix size.


\subsubsection{Effect of matrix shape}
To assess the influence of matrix shape, I compare the median computational time of rSVD on wide matrices ($t_{wide}$) versus tall matrices ($t_{tall}$) by looking at the ratio $t_{wide} / t_{tall}$, see Fig~\ref{wideVsTall} and Table~\ref{tab:times_tall_wide}. For small target ranks, rSVD has a fast runtime on wide matrices across all matrix sizes. The crossover, where rSVD becomes faster on tall matrices, decreases with increasing matrix size, occurring at ranks 22, 14, and 8, for small, medium, and large matrices, respectively. For small matrices, once this crossover rank is exceeded, the relative advantage of tall matrices grows as $k$ approaches 50. For medium sized matrices, the relative speedup on tall matrices remains approximately constant once the rank surpasses the crossover point. For large matrices, the advantage of tall matrices diminishes as $k$ increases passed the crossover point.
\input{tables/times_tall_wide.tex}


\subsection{Approximation Error}
\input{figures/errors_all.tex}

The computation of error is done using the $\ell_2$ operator norm, $\|A - A_k\|_2$, where $A_k$ denotes the rank-$k$ approximation of $A$. For each combination of matrix size and spectral decay profile, the average error is computed across all generated test matrices. Since transposing a matrix does not change its singular values, and since the rSVD produces essentially identical approximations on $A$ and $A^T$, all error analyses are reported for the wide data only. The results are summarized in Fig.~\ref{errorsAll} and Table~\ref{tab:errors}, showing the three spectral decay profiles (slow, moderate, rapid) and the three matrix sizes ($n_s$, $n_m$, $n_l$).
\input{tables/errors.tex}


\subsubsection{Effect of spectral decay}
Fig.~\ref{errorsAll} shows the absolute approximation error (top row) and the relative error of rSVD compared with SVD (bottom row). As expected, the absolute error is largest for matrices with slowly decaying singular values, both for the rSVD and the SVD, with the errors decreasing as $k$ increases. For rapidly decaying spectra, the approximation errors drops much more quickly with increasing rank.

In terms of relative error, rSVD performs worst under rapid spectral decay, although the relative error levels off as $k$ approaches 50. For slow decay, however, the relative error increases approximately linearly with $k$. For the moderate decay case, a middle ground is observed: where initially more linear increase is seen, which start to level off when $k$ gets closer to 50.


\subsubsection{Effect of matrix size}
To assess the effect of matrix size, approximation errors of different matrix sizes are shown in Fig.~\ref{errorsAll}, and relative errors for selected ranks are given in Table~\ref{tab:errors}. As expected, the errors increases with both the matrix size and the target rank $k$. On the contrary, the relative error between the rSVD and the SVD is largest for small matrices, and decreasing with size. Moreover, the difference between slow-, moderate-, and fast-decaying spectra is largest for small matrices: slowly decaying singular values produce significantly higher relative errors.


\section{Discussion}\label{sec:discussion}

\subsection{Summary of Findings}
% short recap of key performance + error trends

\subsection{Interpretation of Computational Performance}
% explain why large matrices help rSVD
% why wide vs tall differs
% why crossover rank behaves as observed

\subsection{Interpretation of Approximation Error}
% connect slow/rapid decay behavior to theory
% explain why relative error varies differently
% discuss influence of k and decay rate

\subsection{Practical Implications}
% when to use rSVD, when not
% guidance on selecting k or understanding decay

\subsection{Limitations and Future Directions}
% synthetic data, p=5, no power iterations, etc.


\section{Conclusion}\label{sec:conclusion}
% Paragraph 1: What, why and setup
% Paragraph 2: Computational performance, errors (absolute, relative), key crossover or threshold behaviors
% Paragraph 3: Practical takeaways (when rSVD, when SVD, etc.)
% Paragraph 4: Future work
This study investigated\dots

The results showed that\dots

From a practical perspective\dots

Future work may consider\dots

\bibliographystyle{plain}  % or 'unsrt', 'alpha', 'ieeetr', etc.
\bibliography{references}


\section*{Appendix}


\paragraph{Operator norm}\label{MatrixNorms}
The $\ell_2$ operator norm of $A\in\mathbb R^{m\times n}$ is the smallest $c\in\mathbb R$ such that $\|Ax\|\le c\|x\|$ for all $x\in \mathbb R^n$, denoted $\|A\|_2$. It is a well known result that
\begin{equation*}
    \|A\|_2 = \sup_{\|x\| = 1}\|Ax\|,
\end{equation*}
for instance, see Lemma 2.7-2 in \cite{kreyszig_functional_1991}.


\paragraph{QR factorization}\label{QRFact}
The QR factorization of $A\in\mathbb R^{m\times b}$ is a decomposition $A = QR$, where $Q\in\mathbb R^{m\times n}$ has orthonormal columns and $R\in\mathbb R^{n\times n}$ is upper diagonal. The matrix $Q$ forms an orthonormal basis for $\text{range}(A)$.

\end{document}
