\documentclass[conference]{IEEEtran}
%\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage[hidelinks]{hyperref}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\graphicspath{{../figures/}}
\def\BibTeX{
    {\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}
}

\begin{document}

\title{On the Accuracy of Randomized Low-Rank Matrix Approximations}

\author{
    \IEEEauthorblockN{
    Elias J. Severinsen\IEEEauthorrefmark{1},
    Ole L. Elvetun\IEEEauthorrefmark{2}}
    \IEEEauthorblockA{
    \textit{Department of Mathematics} \\
    \textit{Norwegian University of Life Sciences (NMBU)} \\
    Ås, Norway \\
    \{elias.jegervatn.severinsen, ole.elvetun\}@nmbu.no}
}

\maketitle


\begin{abstract}
Low-rank matrix approximations are essential in modern applications like scientific computing and data analysis. In recent years the volume of data has seen a rapid increase, resulting in a need for efficient and accurate low-rank approximations. The computation of the singular value decomposition (SVD) is fundamental within low-rank approximations, but is computationally infeasible as the data size increases. Randomized SVD (rSVD) provides an efficient solution, approximating the SVD by first projecting the data onto a low-dimensional space before performing the SVD, significantly reducing the computation time. In this paper we analyze the computation time on data of wide and tall format, comparing the two cases, and analyze the error on matrices with different properties. Our results show that rSVD substantially reduces computation time for large matrices, especially for small target ranks, while achieving good approximation accuracy that depends on the spectral decay of the matrix. We further show that the relative performance on wide versus tall matrices depends on both matrix size and target rank.
\end{abstract}


\section{Introduction}
% Motivation: big data
The need for efficient and accurate numerical algorithms for data analysis is increasingly relevant. Modern scientific and industrial applications produce data sets of massive scale and complexity. In physics simulations, discretization of partial differential equations can result in linear systems with billions of unknowns \cite{ohana_well_2025}. In medical image analysis, large-scale studies produce data sets with hundreds of thousands of images \cite{bustos_medical_2020}. Similarly, in natural language processing, document-term matrices can have tens of thousands of columns and hundreds of rows \cite{antonellis_exploring_2006}. Indeed, large-scale data sets naturally occur in a wide range of fields and applications.

% Introducing the SVD, its challenges, and the rSVD
The SVD is a fundamental tool in data analysis and numerical linear algebra. It has numerous applications, including dimensionality reduction and high-dimensional data visualization \cite{gewers_principal_2021}, data compression \cite{mathews_image_nodate}, and various regularization techniques \cite{hansen_discrete_2010}. Given a matrix $A\in\mathbb R^{m\times n}$, the SVD decomposes the matrix into its main directions of variation, the singular vectors, and a set of singular values describing the importance of the identified directions. However, the computational cost of the SVD is high: for an $n\times n$ matrix, the cost scales as $\mathcal O(n^3)$, which makes it infeasible for many large scale applications. This has motivated the development of randomized algorithms, notably randomized SVD, which provides an efficient alterative at the cost of somewhat lower accuracy.

The rSVD algorithm computes a low-rank approximation of a matrix $A$ by sampling its column space with random test vectors to construct an approximate orthonormal basis $Q$. The matrix is projected onto this approximate subspace, producing a matrix $B$ of smaller size, which captures the main components of $A$. Performing the SVD on this reduced version of $A$ gives an efficient way to compute a rank-$k$ approximation.

% The goal of this paper
In this work, we investigate the trade-offs between the classical SVD and its randomized version. The goal is to compare the computational efficiency and accuracy of the rSVD relative to the SVD on a range of matrix shapes, sizes and spectral decay profiles (the distribution of singular values). We use synthetic test matrices, with a prescribed spectral decay, to investigate the effect on the error of the rSVD against the SVD. In addition, we compare the performance of the rSVD on both tall and wide matrices, evaluating how its efficiency varies with matrix dimensions and target approximation rank.


\section{Theory}\label{sec:theory}


\subsection{Problem formulation}
Consider the goal of finding a rank-$k$ approximation to an $m \times n$ matrix $A$. We are seeking a matrix $X$ with $\text{rank}(X) \le k$ which approximates $A$. The problem formulation is
\begin{equation}\label{probForm1}
    \min_{\text{rank}(X)\le k} \|A - X \|_2,
\end{equation}
where $\| \cdot \|_2$ is the $\ell_2$ operator norm, also known as the spectral norm. Equivalently, every rank-$k$ matrix can be viewed as the projection of $A$ onto a $k$-dimensional subspace of $\mathbb R^m$. 

Let $Q\in\mathbb R^{m\times k}$ have orthonormal columns, i.e. $Q^TQ = I_k$. Then $QQ^TA$ is the orthogonal projection of $A$ onto the $k$-dimensional subspace $\text{span}(Q)$. The problem can also be formulated as
\begin{equation}\label{probForm2}
    \min_{Q^TQ=I_k} \|A-QQ^TA \|_2.
\end{equation}


\subsection{Singular value decomposition}
The singular value decomposition of a matrix $A\in\mathbb R^{m\times n}$ is a factorization of the form
\begin{equation}
    A = U\Sigma V^T,
\end{equation}
where $U$ is an $m \times m$ orthogonal matrix, $\Sigma$ is an $m\times n$ diagonal matrix with non-negative values, called the singular values of $A$, and $V$ is an $n \times n$ orthogonal matrix. The singular values are ordered such that $\sigma_1\ge\sigma_2\ge\dots\ge\sigma_n$. Every matrix has such a factorization, where $\Sigma$ is uniquely determined, but $U$ and $V$ are not \cite{lay_linear_2022}. The columns of $U$ and $V$ are called the left and right singular vectors of $A$, respectively.


\subsection{QR factorization}
The QR factorization of $A\in\mathbb R^{m\times n}$ is a decomposition
\begin{equation}
    A = QR,
\end{equation}
where $Q\in\mathbb R^{m\times n}$ has orthonormal columns and $R\in\mathbb R^{n\times n}$ is upper diagonal. The matrix $Q$ forms an orthonormal basis for $\text{range}(A)$, as described in Section 6 of Golub and Van Loan \cite{golub_matrix_1996}. The computation of a QR factorization is done through a series of orthogonal transformations, like Givens rotations or Householder transformations (see Appendix~\ref{A:givens_rotation}, \ref{A:householder_transformation}).


\subsection{Randomized SVD}
The rSVD is a technique used to approximate the rank-$k$ approximation of an $m\times n$ matrix without computing the SVD. The core idea is to use random sampling to find a low-rank matrix $Q$ which approximates the range of $A$, project $A$ onto $\operatorname{span}(Q)$, and compute the SVD of the projection. The projection $B=Q^TA$ will with a high probability capture the dominant directions of $A$, and the SVD of $B$ will yield a good approximation \cite{halko_finding_2011}. This procedure is summarized in Algorithm \ref{alg:rsvd}. The construction of an orthonormal basis $Q$ of $Y$ is done through a QR factorization. Note that the approximation $A\approx QQ^T A$ and the approximate factorization $A\approx U\Sigma V^T$ produced by rSVD have the same error. This can be seen by observing that $QQ^TA = QB = Q\tilde U\Sigma V^T$, and setting $U=Q\tilde U$.
\begin{algorithm}[!h]
\caption{rSVD}
\label{alg:rsvd}
\begin{algorithmic}[1]
    \Require $A\in\mathbb R^{m\times n}$, target rank $k$, oversampling parameter $p$.
    \Ensure Approximate rank-$k$ factorization $U\Sigma V^T$.
    \State Generate a random $n\times(k+p)$ test matrix $\Omega$.
    \State Compute $Y = A\Omega$.
    \State Form an orthonormal basis $Q$ of $Y$.
    \State Set $B = Q^TA$.
    \State Compute the SVD $B = \tilde U\Sigma V^T$.
    \State Set $U = Q\tilde U$. 
\end{algorithmic}
\end{algorithm}


\subsection{Error estimates}
In 1936, C. Eckart and G. Young showed that the solution to Equation~\eqref{probForm1} is $X = U_k\Sigma_k V_k^T$, where $U_k$ and $V_k$ consist of the $k$ first left and right singular vectors of $A$, respectively, and $\Sigma_k$ contains the $k$ first singular values of $A$ \cite{eckart_approximation_1936}. This is known as the Eckart–Young–Mirsky theorem. Alternatively, the solution $Q$ of Equation~\eqref{probForm2} consists of the $k$ first left singular values of $A$. This implies that
\begin{equation}\label{eq:svd_error}
    \min_{\text{rank}(X)\le k} \|A - X \|_2 = \sigma_{k+1},
\end{equation}
where $\sigma_{k+1}$ is the $(k+1)$th singular value of $A$. Equation \eqref{eq:svd_error} says that no rank-$k$ approximation of $A$ can have an error smaller than $\sigma_{k+1}$.

Let $A\in\mathbb R^{m\times n}$, and pick a target rank $k\ge 2$ and an oversampling parameter $p\ge 2$, where $p+k\le\min\{m,n\}$. Draw a random matrix $\Omega\in\mathbb R^{m\times (k+p)}$, with independent and identically distributed (IID) elements from the standard normal distribution, and use Algorithm~\ref{alg:rsvd} to obtain a rank-$k$ approximation $X = U\Sigma V^T$. Then,
\begin{equation}\label{eq:rsvd_error}
\begin{split}
    \mathbb E\|A - X\|_2 \le& \\
    \left( 1 + \sqrt{\frac{k}{p-1}} \right)\sigma_{k+1} +
    \frac{e\sqrt{k+p}}{p}\sqrt{\sum_{j=k+1}^{\min\{m,n\}}\sigma_j^2},
\end{split}
\end{equation}
where $\sigma_j$ is the $j$th singular value of $A$, $e$ is Euler's constant, and $\mathbb E$ denotes the expected value. For a proof, see Theorem 10.5 in Halko et al. \cite{halko_finding_2011}.

Noting that $\sigma_j \le \sigma_{k+1}$ for all $j > k + 1$, Equation~\eqref{eq:rsvd_error} implies that
\begin{equation}\label{eq:rsvd_error_simple}
    \mathbb E\|A - X\|_2 \le \gamma_k \sigma_{k+1},
\end{equation}
where $\gamma_k$ is a constant given by $\gamma_k = (1 + \tfrac{4\sqrt{k+p}}{p-1}\min\{m,n\})$ and $\sigma_{k+1}$ is the ($k+1$)th singular value of $A$.


\subsection{Computational cost}
Assume that $A\in\mathbb R^{m\times n}$. Standard implementations of the SVD in linear algebra libraries like LAPACK or ScaLAPACK are based on two-stage algorithms \cite{blanchard_2019_nlafet}. In the first stage, $A$ is reduced to a bi-diagonal, most commonly through Householder transformations (see Appendix~\ref{A:householder_transformation}). In the second stage, the SVD of the bi-diagonal matrix is computed, typically using a QR-based or a divide-and-conquer approach, as described in Section 2 in \cite{blanchard_2019_nlafet}. The LAPACK routine GESDD, used in this study, where `GE' denotes a general dense matrix and `SDD' identifies the SVD driver, implements the divide-and-conquer method \cite{LAPACK99}. This method typically achieves higher performance compared to the QR-based approach \cite{dongarra_singular_2018}. The cost of this process is of the order $\mathcal O(mn\min\{m,n\})$, which simplifies to $\mathcal O(m^2n)$ for wide matrices where $m < n$, or to $\mathcal O(mn^2)$ for tall matrices where $m > n$.

The rSVD algorithm consists of two stages. In the first stage, an approximate orthogonal basis $Q$ of $A$ is constructed, and in the second stage, the SVD is approximated using the projection of $A$ onto $\operatorname{span}(Q)$. The first stage of Algorithm \ref{alg:rsvd}, which constructs the orthogonal basis $Q$, has a cost of $\mathcal O(mk^2)$ as a result of the QR factorization \cite{Buttari_2008}. Stage two, the projection and construction of an approximate SVD, has a cost of $\mathcal O(mnk)$ and $\mathcal O(nk^2)$ from the projection $B=Q^TA$ and the SVD $B=\tilde U\Sigma V^T$, respectively \cite{halko_finding_2011}. The overall cost is therefore on the order of $\mathcal O(mnk)$. More efficient but less accurate algorithms for stage 2 are proposed, for instance Algorithm 5.2 in \cite{halko_finding_2011}, which relies on the interpolative decomposition (ID) (see Appendix~\ref{A:interpolative_decomposition}), and has a cost of $\mathcal O(mn\log k)$.


\section{Methods}\label{sec:methods}


\subsection{Test matrix generation}
To compare the performance of the SVD and the rSVD on matrices with different spectral decay profiles, we generated random test matrices $A=U\Sigma V^T$, where $U$ and $V$ are random orthogonal matrices, and $\Sigma$ is a diagonal matrix with prescribed singular values $\{\sigma_1, \dots, \sigma_n\}$.

Each orthogonal matrix was constructed by drawing a random $n\times n$ matrix $\Omega$ with independent elements from the standard normal distribution, performing a QR decomposition $\Omega = \tilde QR$, and correcting the signs of the orthogonal columns with $Q = \tilde Q\Lambda$, where $\Lambda = \operatorname{diag}(\operatorname{sign}(R_{ii}))$, so that the diagonal elements of $R$ are positive. This procedure generated random orthogonal matrices uniformly distributed over the orthogonal group (Haar measure) \cite{mezzadri_how_2007}.

To investigate how spectral decay affects accuracy, three sets of prescribed singular values were used. We used the polynomially decaying sequence of the form
\begin{equation}\label{decay}
    \sigma_{k+1} = \frac{10}{(1 + \alpha k)^2},\quad k=0,\dots,n-1,
\end{equation}
where the parameter $\alpha$ controls the rate of decay. The condition number is $\kappa = \sigma_1 / \sigma_{n} = (1 + \alpha n - \alpha)^2$, and solving for $\alpha$ gives
\begin{equation}
    \alpha = \frac{\sqrt{\kappa(n-1)^2}-n+1}{(n-1)^2}.
\end{equation}
In this study, we used the condition numbers $\kappa_1 = 2$, $\kappa_2 = 50$, and $\kappa_3 = 1000$, which corresponds to slow, moderate and a rapid spectral decay (see Fig.~\ref{spectralDecay}). The singular values from Equation~\eqref{decay} are placed on the diagonal of $\Sigma$, and the test matrix is the result of $A = U\Sigma V^T$.

The size of the test matrices were set to $n\times 5n$, for the values $n_s=100$, $n_m=500$, and $n_l=1000$, which we refer to as the small, medium and large case, respectively. For each combination of spectral decay (slow, moderate, rapid) and matrix size (small, medium, large), We generated $r$ random samples depending on the size, where $r=30$ for the small case, $r=15$ for the medium case, and $r=5$ for the large case. In addition, an oversampling parameter of $p=5$ was used for the rSVD in all experiments, as suggested by Halko et al. \cite{halko_finding_2011}.


\subsection{Computational environment}
All experiments were performed on a laptop running Fedora Linux 43 with Linux kernel version 6.17. The system was running on an Intel(R) Core(TM) i7-10510U (4 cores, 8 threads) with 16GB of memory.

Python 3.13 was used for all computations, together with NumPy and SciPy for linear algebra routines \cite{2020NumPy-Array,2020SciPy-NMeth}. The implementation of Algorithm~\ref{alg:rsvd} was done using the QR and SVD routines provided by SciPy, which were specified to use the GESDD solver for SVD computations. All library specifications and code used to generate the results in this paper is openly available at: \href{https://github.com/EliasSev/randomized-svd-paper}{github.com/EliasSev/randomized-svd-paper}.

\input{figures/spectral_decay.tex}  % Fig 1


\section{Results}\label{sec:results}
\input{figures/run_times_absolute.tex}  % Fig 2
\input{figures/wide_vs_tall.tex}  % Fig 3


\subsection{Computational performance}
When assessing the effect of the matrix size on the computational time, we computed the median value across all test matrices of a given size and across all three spectral decay sets, since the condition number of a dense matrix does not affect the computational cost of the SVD or the QR factorization used in the rSVD \cite{blanchard_2019_nlafet,Buttari_2008}. For each matrix size, we reported the median value over 90 (small), 45 (medium), and 15 (large) trials. The results are presented in Fig.~\ref{fig:run_times_absolute}, and relative runtimes for selected target ranks are listed in Tables~\ref{tab:times_all} and \ref{tab:times_tall_wide}.

\input{tables/times_all.tex}  % TABLE I


\subsubsection{Effect of matrix size}
Fig.~\ref{fig:run_times_absolute} shows the median run times of rSVD as a function of target rank $k$ compared with the median SVD run time, and Table~\ref{tab:times_all} shows the speedup of rSVD compared to SVD. For small matrices ($n_s=100$), rSVD is significantly faster than SVD for small ranks ($k<10$), up to 13 times speedup. As $k$ increases, the computational time of rSVD increases rapidly, and SVD is faster when $k$ is greater than 10. For medium ($n_m=500$) and large ($n_l=1000$) matrices, rSVD consistently performs faster than SVD across all ranks used in this study (5 to 50). Speedups between 7 and 32 is seen for large matrices and between 1.5 and 20 for medium matrices. The relative advantage of rSVD over SVD generally increases with the matrix size.


\subsubsection{Effect of matrix shape}
To assess the influence of matrix shape, we compared the median computational time of rSVD on wide matrices ($t_{wide}$) versus tall matrices ($t_{tall}$) by looking at the ratio $t_{wide} / t_{tall}$, see Fig~\ref{wideVsTall} and Table~\ref{tab:times_tall_wide}. For small target ranks, rSVD has a fast runtime on wide matrices across all matrix sizes. The crossover, where rSVD becomes faster on tall matrices, decreases with increasing matrix size, occurring at ranks 22, 14, and 8, for small, medium, and large matrices, respectively. For small matrices, once this crossover rank is exceeded, the relative advantage of tall matrices grows as $k$ approaches 50. For medium sized matrices, the relative speedup on tall matrices remains approximately constant once the rank surpasses the crossover point. For large matrices, the advantage of tall matrices diminishes as $k$ increases passed the crossover point.
\input{tables/times_tall_wide.tex}  % TABLE II


\subsection{Approximation Error}
\input{figures/errors_all.tex}
The approximation error is measured in the $\ell_2$ operator norm, $\|A - A_k\|_2$, where $A_k$ denotes the rank-$k$ approximation of $A$. For each combination of matrix size and spectral decay profile, the average error is computed across all generated test matrices. Since transposing a matrix does not change its singular values, and since in our experiments the approximation errors obtained by applying rSVD to $A$ and $A^T$ were numerically identical, all error analyses are reported for the wide data only. The results are summarized in Fig.~\ref{errorsAll} and Table~\ref{tab:errors}, showing the three spectral decay profiles (slow, moderate, rapid) and the three matrix sizes ($n_s$, $n_m$, $n_l$).
\input{tables/errors.tex}  % TABLE III


\input{figures/expected_error.tex}
\subsubsection{Effect of spectral decay}
Fig.~\ref{errorsAll} shows the absolute approximation error (top row) and the relative error of rSVD compared with SVD (bottom row). As expected, the absolute error is largest for matrices with slowly decaying singular values, both for the rSVD and the SVD, with the errors decreasing as $k$ increases. For rapidly decaying spectra, the approximation errors drops much more quickly with increasing rank.

In terms of relative error, rSVD performs worst under rapid spectral decay, although the relative error levels off as $k$ approaches 50. For slow decay, however, the relative error increases approximately linearly with $k$. For the moderate decay case, a middle ground is observed: where initially more linear increase is seen, which start to level off when $k$ gets closer to 50.


\subsubsection{Effect of matrix size}
To assess the effect of matrix size, approximation errors of different matrix sizes are shown in Fig.~\ref{errorsAll}, and relative errors for selected ranks are given in Table~\ref{tab:errors}. As expected, the errors increases with both the matrix size and the target rank $k$. On the contrary, the relative error between the rSVD and the SVD is largest for small matrices, and decreasing with size. Moreover, the difference between slow-, moderate-, and fast-decaying spectra is largest for small matrices: slowly decaying singular values produce significantly higher relative errors.


\section{Discussion}\label{sec:discussion}


\subsection{Interpretation of computational performance}
% Why large matrices and small k help rSVD
The results show that rSVD achieves its greatest speedup compared to the SVD on matrices of large size, and this speedup is greatest for small target ranks $k$ (Fig.~\ref{fig:run_times_absolute} and Table~\ref{tab:times_all}). This can be explained by the computational complexity of the rSVD, $\mathcal O(kn^2)$, compared to SVDs complexity of $\mathcal O(n^3)$, assuming a matrix of shape $n\times 5n$. As $k$ approaches $n$, the complexity will be approximately $\mathcal O(n^3)$, and the additional computations in the rSVD are no longer beneficial, as was seen for small matrices with target ranks close to 50.

% Why for small matrices, a sudden increase for rSVD
For small matrices, rSVD saw a sudden increase in computational runtime (see Table~\ref{tab:times_all}). This behavior may be related to implementation details, such as how the QR decomposition or the SVD within the rSVD algorithm is computed. A possible factor is that the matrix size crossed a threshold causing the computations to no longer fit in the fast CPU cache \cite{lam_cache_1991}. Further profiling would be required to confirm the exact cause.

% Why for small k, wide matrices are best
Additionally, for small target ranks $k$, rSVD performs faster on wide matrices than on tall matrices. This can be understood by looking at the dominant costs of Algorithm~\ref{alg:rsvd}.
The first stage constructs an approximate orthonormal basis $Q$ and involves performing a QR-decomposition on the $m\times k$ matrix $Y$, at a cost of $\mathcal O(k^2m)$. The second stage involves forming the projected matrix $B = Q^T A$ with cost $\mathcal O(kmn)$, and computing the SVD of $B$, which costs $\mathcal O(k^2 n)$.
Although the overall cost of Algorithm~\ref{alg:rsvd} scales as $\mathcal O(kmn)$, the contribution of the quadratic terms $\mathcal O(k^2m)$ and $\mathcal O(k^2n)$ decides whether tall or wide matrices are more efficient. Specifically, for wide matrices where $m \ll n$, the SVD of $B$ dominates and $\mathcal O(k^2n)$ is greater than $\mathcal O (k^2m)$. Conversely, for tall matrices where $m \gg n$, the QR decomposition dominates. The observed crossover rank, at which tall matrices become faster, is a balance between these two terms, and depends on both the matrix dimensions and the target rank $k$.


\subsection{Interpretation of approximation error}
As shown in Fig.~\ref{errorsAll}, the approximation error of both the rSVD and SVD depends on spectral decay and target rank $k$. Rapidly decaying spectra give low-rank approximations with smaller errors, whereas slowly decaying spectra require much larger $k$ to achieve comparable accuracy. For the truncated SVD, this is a direct result of the Eckart–Young–Mirsky theorem, as expressed in Equation~\eqref{eq:svd_error}. For the rSVD, the error bound in Equation~\eqref{eq:rsvd_error_simple} tells us the same: When the singular values decay slowly, the $\sigma_{k+1}$ term remains large, resulting in a larger theoretical upper bound on the expected approximation error.

Fig.~\ref{expectedError} further shows that the bound in Equation~\eqref{eq:rsvd_error} is highly conservative, especially under slow and moderate spectral decay. The predicted error bound is between one and three orders of magnitude larger than the observed error. For rapidly decaying spectra, the bounds are somewhat tighter, but still overestimates the true error by roughly one order of magnitude. 

To understand why the bounds behave this way, it is helpful to investigate the structure of
\begin{equation}
    Y = A\Omega,
\end{equation}
which forms the sample matrix in Algorithm~\ref{alg:rsvd}. Let $A=U\Sigma V^T$ be the SVD of $A$, and partition $\Sigma$ as $\Sigma = \operatorname{diag}(\Sigma_1, \Sigma_2)$, where $\Sigma_1$ contains the $k+1$ first singular values and $\Sigma_2$ contains the remainder. Correspondingly, partition the right singular vectors as $V = \begin{bmatrix}V_1 & V_2\end{bmatrix}$. Then $A$ can be expressed as
\begin{equation}\label{eq:Y_partition}
    A =
    U
    \begin{bmatrix}
    \Sigma_1 & \\
    & \Sigma_2
    \end{bmatrix}
    \begin{bmatrix}
    V_1^T \\ V_2^T
    \end{bmatrix}.
\end{equation}
Multiplying by the random test matrix $\Omega$ gives
\begin{equation}\label{eq:rsvd_perturbation}
    Y = 
    U
    \begin{bmatrix}
    \Sigma_1\Omega_1 \\
    \Sigma_2\Omega_2
    \end{bmatrix},
\end{equation}
where $\Omega_1 = V_1^T\Omega$ and $\Omega_2 = V_2^T\Omega$. Intuitively, $\Sigma_1\Omega_1$ contains the dominant actions of $A$, while $\Sigma_2\Omega_2$ represents perturbation. When the singular values decay slowly, the perturbation $\Sigma_2\Omega_2$ remains relatively large, resulting in a larger upper bound on the expected error. For a more comprehensive discussion of rSVD error bounds, see the discussion in Chapter 9 of Halko et al. \cite{halko_finding_2011}.

Further, as noted in the Results section, the rSVD approximation errors for $A$ and $A^T$ were numerically identical in our experiments. This can be understood by comparing the random sampling steps $Y = A\Omega$ and $Y' = A\tilde\Omega$. Similarly to Equation~\eqref{eq:rsvd_perturbation}, the actions of $A^T$ is captured in $\Sigma_1\tilde\Omega_1$, where $\tilde\Omega_1 = U_1^T\tilde\Omega$. Hence, for $A^T$, the quality of the reconstruction depends on the sampling of $U$, whereas for $A$ it depends on the sampling of $V$. In our experiments, both $U$ and $V$ were random orthogonal matrices, and therefore the sampling $Y$ and $Y'$ gives numerically identical rank-$k$ approximations using rSVD.


\subsection{Interpretation of the relative approximation error}
The relative error between rSVD and the SVD approximation (Fig.~\ref{errorsAll}, bottom row) was largest for small matrices and smallest for large ones. This trend is consistent with Equation~\eqref{eq:rsvd_perturbation}: The perturbation term $\Sigma_{2}\Omega_{2}$ grows with the dimension of $\Omega$, which increases the numerator of the relative error. The denominator, however, is the SVD error $\sigma_{k+1}$, which does not depend on the matrix size. Consequently, as the problem dimension increases, the perturbation grows while the reference error remains fixed, resulting in an increase in the relative error bound, even though the relative error decreases in practice.

Regarding the influence of spectral decay, the experiments showed that a rapid decay gave the highest relative error. This behavior should be interpreted alongside the absolute: although the relative error is greater for rapid decay, the absolute error is the smallest for such spectra. This illustrates how rSVD and truncated SVD both perform best when the dominant amount of actions of $A$ are concentrated in its first few singular vectors, even if small differences appear magnified when expressed relative to the SVD.


\subsection{Practical implications}
The observed performance properties of rSVD have several direct implications for practical use in machine learning and scientific computing. The dependence of computational cost on both the matrix shape and the target rank suggests the choice between SVD and rSVD should not be based solely on matrix size, but rather on the interaction between $k$, $m$, and $n$. In particular, the fact that the matrix shape influences performance shows that transposition strategies used by some libraries can meaningfully affect runtime.

These considerations are reflected partially in the heuristics implemented in Scikit-learn. For example, \texttt{TruncatedSVD} relies on randomized SVD by default, consistent with the common scenario in which only a small number of components is required \cite{sklearn_tsvd}. Likewise, Scikit-learn's \texttt{PCA} defaults to rSVD when the data are sufficiently large and the target dimension is significantly smaller than the matrix dimension \cite{sklearn_pca}. Both these methods rely on Scikit-learn's \texttt{randomized\_svd} \cite{sklearn_randomized_svd}. Notably, this implementation transposes the data when the number of columns exceeds the number of rows. However, our results indicate that this strategy is beneficial only above certain rank thresholds and can be suboptimal for small $k$.

From a user perspective, our findings suggest several guidelines. Randomized SVD is most effective when (i) the matrix size is large, (ii) the target rank is small relative to the matrix dimensions, and (iii) the singular values decay quickly. Deterministic SVD may be preferable for small matrices or when high approximation accuracy is critical. 


\subsection{Limitations and future directions}
This study focused on synthetic data without structure in order to isolate the effects of matrix shape, size and spectral decay on the approximation error of rSVD. Real-world data may have additional properties, such as sparsity, symmetry, or noise. For instance, sparse matrices appear in natural language processing \cite{antonellis_exploring_2006}, graph theory \cite{george_graph_2012}, and in discretization of partial differential equations (PDEs) \cite{olver_introduction_2014}. Specialized algorithms which take advantage of such structures exist for efficiently computing the SVD \cite{saad_iterative_2003}. Additionally, iterative methods are developed which avoid explicit computation of singular vectors and values entirely, which have important applications in the field of inverse problems \cite{hansen_discrete_2010}.

Further, Algorithm~\ref{alg:rsvd} may be extended to include power iterations (see Algorithm 4.3 in \cite{halko_finding_2011}), which are known to yield substantially more accurate approximations, particularly for matrices with slowly decaying singular spectra. Likewise, the oversampling parameter $p$, which was fixed to $p=5$ in this study, may be increased to further improve accuracy of the approximations. Both enhancements, however, increase the computational cost, and their benefits must therefore be weighed against the additional runtime.


\section{Conclusion}\label{sec:conclusion}
% What, why and setup. Results
This work investigated the computational efficiency and approximation accuracy of rSVD relative to the classical SVD across matrices of varying size, shape, and spectral decay. Our results showed that the computational time for rSVD is substantially lower than that of the classical SVD for large matrices, especially for small target ranks. The accuracy of rSVD is largely determined by the spectral decay, where a rapid decay yields better approximations. We also observed that the crossover rank, at which rSVD becomes faster for tall matrices than for wide ones, depends on both matrix size and target rank. Generally, for small target ranks, matrices of wide format are preferred, where this crossover rank was seen to decrease with the size of the matrices. 

% Practical takeaways
Our findings suggest possible improvements of the heuristics in the rSVD implementation in libraries such as Scikit-learn, which should not only depend on matrix shape, but also the target rank and matrix size. For a user perspective, we saw that rSVD is most effective for small target ranks on large matrices, especially with rapid spectral decays.

% Future work
Limitations of our experiments include the use of synthetic dense matrices, a fixed oversampling parameter, and no power iterations. Future work could explore real-world data, sparse matrices, and alternative algorithms for the rSVD. Overall, rSVD is an effective and computationally efficient alternative to classical SVD in many practical scenarios, especially for low-rank approximations of large matrices.


\bibliographystyle{plain}
\bibliography{references}


\appendix
\renewcommand{\theequation}{A\arabic{equation}}
\setcounter{equation}{0}


\subsection{Operator norm}\label{A:operator_norm}
The $\ell_2$ operator norm of $A\in\mathbb R^{m\times n}$ is the smallest $c\in\mathbb R$ such that $\|Ax\|\le c\|x\|$ for all $x\in \mathbb R^n$, denoted $\|A\|_2$. It is a well known result that
\begin{equation}\label{l2_1}
    \|A\|_2 = \sup_{\|x\| = 1}\|Ax\|,
\end{equation}
for instance, see Lemma 2.7-2 in Kreyszig \cite{kreyszig_functional_1991}. Additionally, it can be shown that
\begin{equation}\label{l2_2}
    \|A\|_2 = \sigma_1,
\end{equation}
where $\sigma_1$ is the largest singular value of $A$. See Theorem 6 Section 7.2 in Lay et al. for a proof \cite{lay_linear_2022}. Equation \eqref{l2_2} is used when computing $\|A\|_2$ in practice.


\subsection{Givens rotation}\label{A:givens_rotation}
Givens rotation is a rotation in the plane by an angle $\theta$. The rotation is represented by a matrix of the form
\begin{equation}
    G = 
    \begin{bmatrix}
    c & -s \\
    s & c
    \end{bmatrix},
\end{equation}
where $c = \cos\theta$ and $s = \sin\theta$. The method is named after Wallace Givens who introduced it in the 1950s. The matrix can be extended to act on rows $i$ and $j$ of a matrix $A$, and used repeatedly to convert a matrix into a lower triangular form in order to compute a QR factorization, see Section 5.2 of Golub and Val Loan \cite{golub_matrix_1996}.


\subsection{Householder transformation}\label{A:householder_transformation}
Let $v$ be a normal vector defining a hyperplane $\mathcal H$. The Householder transformation of a point $x$ is a linear transformation $x\mapsto Px$ representing a reflection about $\mathcal H$, where $P$ is the Householder matrix, given by 
\begin{equation}
    P = I - 2uu^T,\quad u = \frac{v}{\|v\|}.
\end{equation}
The transformation was described by Alston S. Householder in a 1958 paper \cite{householder_unitary_1958}. Householder transformations can be used to reduce matrices to bi-diagonal form, or to lower-triangular form as part of a QR factorization, as described in Sections 5.4 and 5.3 of Golub and Van Loan, respectively \cite{golub_matrix_1996}.


\subsection{Interpolative decomposition}\label{A:interpolative_decomposition}
Let $A\in\mathbb R^{m\times n}$ be of rank $k$. The ID of $A$ is a factorization of the form
\begin{equation}
    A = A_{(:,J)}X,
\end{equation}
where $J\subset \{1,\dots,n\}$ is an index set of $k$ indices, $A_{(:,J)}$ a matrix made up of $J$'s columns of $A$, and $X$ a matrix where $X_{(:,J)}=I_k$. See Section 3.2.3 in Halko et al. \cite{halko_finding_2011}.

\end{document}
